# Третье домашнее задание - итоговый проект.
## Постановка задачи
В файле [hw3_info.pdf](https://github.com/borodatsik/middle-python-edu/blob/main/03_etl_airflow/hw3_info.pdf)

## Предисловие
Apache Airflow устанавливался на домашний ПК с помощью Docker Windows.

## Принцип работы программы
### Основной принцип
DAG состоит из 5 тасков:
1. Таск create_tables - cоздается структура базы данных PostgreSQL в определенной схеме.
1. Таск download_egrul - скачивается архив с данными по юр.лицам - ЕГРЮЛ.
1. Таск upload_telecom_companies - данные из скачанного архива фильтруются (отбираются данные по нужному коду ОКВЭД.
1. Таск upload_vacancies - получаются данные по вакансиям из API HeadHunter, трансформируются, фильтруются и загружаются в базу данных.
1. Таск print_top_key_skills - данные по вакансиям фильтруются по названию работодателя (выбираются только телеком-компании). Далее собирается сводная таблица по количеству встречающихся в этих вакансиях требуемых ключевых навыков, результат печатается в лог таска.

Зависимости тасков выглядят следующим образом:

![task_dependencies](https://github.com/borodatsik/middle-python-edu/blob/main/03_etl_airflow/img/01_task_dependencies.png?raw=true)

* При запуске DAG одновременно запускаются таски create_tables и download_egrul.
* После выполнения create_tables запускается загрузка вакансий upload_vacancies.
* После выполнения create_tables и download_egrul запускается загрузка телеком-компаний в БД upload_telecom_companies.
* И только по окончании upload_vacancies и upload_telecom_companies запускается финальный print_top_key_skills.

### Версии DAG, варианты запуска
Внутри dag.py в цикле генерируется три версии DAG:
1. test
1. preprod
1. prod

Отличия тасков:
1. Таск create_tables выполняется одинаково во всех версиях: если схема и таблицы уже существуют, то схема удаляется целиком вместе с таблицами и создается заново.

	Отличаются только наименования схем (соответственно версиям DAG):
	* hw3_test
	* hw3_preprod
	* hw3_prod

1. Таск download_egrul:
	* в тестовой и препродуктивной версии вместо 15-гигобайтового архива загружается "болванка" из того же сайта - okved_2.json.zip.
	* в продуктивной версии загружается полный архив.
	
1. Таск upload_telecom_companies. Для загрузки данных по телеком-компаниям в БД используются разные варианты архивов ЕГРЮЛ:
	* в тестовой версии используется предварительно подготовленный архив, содержащий только первые 100 .json файлов.
	* в препродуктивной версии используется предварительно скачанный полный архив.
	* в продуктивной версии используется полный архив, скачанный в таске download_egrul.

## Структура программы
### Папки
* airflow - содержимое этой папки помещается в $AIRFLOW_HOME приложения Apache Airflow
	* airflow/dags - 
	* airflow/bulk_data - содержит скачиваемые файлы ЕГРЮЛ. Подключалась как volume к контейнерам Apache Airflow.
	В репозиторий содержимое не загружается.
		* airflow/bulk_data/egrul.json.zip - скачиваемый в тестовой и препродуктивной версии DAG файл ЕГРЮЛ, используемый в BashOperator.
		Файл используется как "заглушка" - фактически на его место скачивался файл меньшего объема - okved_2.json.zip.
		* airflow/bulk_data/egrul_test.json.zip - вручную подготовленный срез полного архива ЕГРЮЛ (150 МБ), содержащий только первые 100 .json файлов.
		Файл используется в тестовой версии DAG в upload_egrul.py - для тестирования загрузки данных по телеком-компаниям в базу данных.
		* airflow/bulk_data/egrul_full.json.zip - предварительно скачанный полный файл ЕГРЮЛ (16 ГБ). Используется в препродуктивной версии DAG.
		* airflow/bulk_data/egrul_full_download.json.zip - скачивающийся в продуктивной версии DAG полный файл ЕГРЮЛ (16 ГБ).

### Дополнительные файлы
* Папка img - скриншоты для настоящего README.md
* hw3_info.pdf - файл-копия с постановкой задачи.







* sql - скрипты SQL по созданию структуры БД.
	Скрипт по созданию базы данных create_database.sql выполняется отдельно, перед запуском приложения.

### PY-файлы
* config.py - конфигурация приложения.
* psql.py - обработка подключения к БД PostgreSQL, функции-надстройки над psycopg2.
* helpers.py - вспомогательные функции.
* hh_html_parsing.py - вариант выполнения домашнего задания 1 - парсинг HTML-страницы.
* hh_api_parsing.py - вариант выполнения домашнего задания 2 - использование API.
* main.py - точка входа.